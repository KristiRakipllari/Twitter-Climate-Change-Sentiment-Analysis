{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kristi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports, installation of flair required\n",
    "import pandas as pd\n",
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings, FlairEmbeddings\n",
    "from flair.data import Sentence\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import warnings\n",
    "import emoji\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "import spacy \n",
    "eng_lemmatizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re # regex\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re, string\n",
    "\n",
    "fasttext = WordEmbeddings('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@tiniebeany climate change is an interesting h...</td>\n",
       "      <td>792927353886371840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @NatGeoChannel: Watch #BeforeTheFlood right...</td>\n",
       "      <td>793124211518832641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fabulous! Leonardo #DiCaprio's film on #climat...</td>\n",
       "      <td>793124402388832256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @Mick_Fanning: Just watched this amazing do...</td>\n",
       "      <td>793124635873275904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @cnalive: Pranita Biswasi, a Lutheran from ...</td>\n",
       "      <td>793125156185137153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  @tiniebeany climate change is an interesting h...   \n",
       "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
       "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
       "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
       "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
       "\n",
       "              tweetid  \n",
       "0  792927353886371840  \n",
       "1  793124211518832641  \n",
       "2  793124402388832256  \n",
       "3  793124635873275904  \n",
       "4  793125156185137153  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"./data/twitter_sentiment_data.csv\")\n",
    "\n",
    "def importpreprocessing(data):\n",
    "    data.drop(columns=\"tweetid\")\n",
    "    data.rename(columns={'sentiment':'label','message':'text'},inplace=True)\n",
    "\n",
    "    return data\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Clean-up Text</h4>\n",
    "\n",
    "- Delete \"RT\"\n",
    "- Delete HTTPS/HTTP Links\n",
    "- Delete Double Whitespaces\n",
    "- Delete @twitteruser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    RE_TWITTERUSERS = re.compile(r'@\\S+', re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_HTTPS = re.compile(r'https\\S+', re.IGNORECASE)\n",
    "    RE_HTTP = re.compile(r'http\\S+', re.IGNORECASE)\n",
    "    RE_RT = re.compile(r'RT', re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TWITTERUSERS, \" \", text)\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "    text = re.sub(RE_HTTPS, \" \", text)\n",
    "    text = re.sub(RE_HTTP, \" \", text)\n",
    "    text = re.sub(RE_RT,\" \",text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Replace emojis with sentiment words\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokinize and Lemmatizise Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(data):\n",
    "    # generate new column with stopword free and lemmatizised text\n",
    "    data[\"shortData\"] = data['cleanData'].apply(lambda x:tokenize(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokenizedArray(sentences):\n",
    "\n",
    "    # Initialize tokenizer and empty array to store modified sentences.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenizedArray = []\n",
    "    for i in range(0, len(sentences)):\n",
    "        # Convert sentence to lower case.\n",
    "        sentence = sentences[i].lower()\n",
    "\n",
    "        # Split sentence into array of words with no punctuation.\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "\n",
    "        # Append word array to list.\n",
    "        tokenizedArray.append(words)\n",
    "\n",
    "    # print(tokenizedArray)\n",
    "    return tokenizedArray  # send modified contents back to calling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [tiniebeany, climate, change, is, an, interest...\n",
       "1    [rt, natgeochannel, watch, beforetheflood, rig...\n",
       "2    [fabulous, leonardo, dicaprio, s, film, on, cl...\n",
       "3    [rt, mick_fanning, just, watched, this, amazin...\n",
       "4    [rt, cnalive, pranita, biswasi, a, lutheran, f...\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedLi = createTokenizedArray(data_test)\n",
    "\n",
    "pd.Series(tokenizedLi[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kristi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def removeStopWords(tokenList):\n",
    "    '''\n",
    "    Create array of words with no punctuation or stop words.\n",
    "    :param tokenList: tokenized list\n",
    "    :return: array of words with no punctuation or stop words.\n",
    "    '''\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    shorterSentences = []  # Declare empty array of sentences.\n",
    "\n",
    "    for sentence in tokenList:\n",
    "        shorterSentence = []  # Declare empty array of words in single sentence.\n",
    "        for word in sentence:\n",
    "            if word not in stopWords:\n",
    "\n",
    "                # Remove leading and trailing spaces.\n",
    "                word = word.strip()\n",
    "\n",
    "                # Ignore single character words and digits.\n",
    "                if (len(word) > 1 and word.isdigit() == False):\n",
    "                    # Add remaining words to list.\n",
    "                    shorterSentence.append(word)\n",
    "        shorterSentences.append(shorterSentence)\n",
    "    return shorterSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence BEFORE removing stop words:\n",
      "['tiniebeany', 'climate', 'change', 'is', 'an', 'interesting', 'hustle', 'as', 'it', 'was', 'global', 'warming', 'but', 'the', 'planet', 'stopped', 'warming', 'for', '15', 'yes', 'while', 'the', 'suv', 'boom']\n",
      "\n",
      "\n",
      "Sample sentence AFTER removing stop words:\n",
      "['tiniebeany', 'climate', 'change', 'interesting', 'hustle', 'global', 'warming', 'planet', 'stopped', 'warming', 'yes', 'suv', 'boom']\n"
     ]
    }
   ],
   "source": [
    "tokenizedNoStopLi = removeStopWords(tokenizedLi)\n",
    "\n",
    "print(f\"Sample sentence BEFORE removing stop words:\\n{tokenizedLi[0]}\")\n",
    "print(f\"\\n\\nSample sentence AFTER removing stop words:\\n{tokenizedNoStopLi[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raname columns and convert label -1 to 3 for conversion into labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemWords(sentenceArrays):\n",
    "    '''\n",
    "    Removes suffixes and rebuilds the sentences.\n",
    "    :param sentenceArrays: stentences list\n",
    "    :return: array of sentences without suffixes\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    stemmedSentences = []\n",
    "    for sentenceArray in sentenceArrays:\n",
    "        stemmedArray = []  # Declare empty array of words.\n",
    "        for word in sentenceArray:\n",
    "            stemmedArray.append(ps.stem(word))  # Add stemmed word.\n",
    "\n",
    "        # Convert array back to sentence of stemmed words.\n",
    "        delimeter = ' '\n",
    "        sentence = delimeter.join(stemmedArray)\n",
    "\n",
    "        # Append stemmed sentence to list of sentences.\n",
    "        stemmedSentences.append(sentence)\n",
    "    return stemmedSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence BEFORE stemming:\n",
      "['tiniebeany', 'climate', 'change', 'interesting', 'hustle', 'global', 'warming', 'planet', 'stopped', 'warming', 'yes', 'suv', 'boom']\n",
      "\n",
      "Sample sentence AFTER stemming:\n",
      "tiniebeani climat chang interest hustl global warm planet stop warm ye suv boom\n"
     ]
    }
   ],
   "source": [
    "stemmedLi = stemWords(tokenizedNoStopLi)\n",
    "\n",
    "print(f\"Sample sentence BEFORE stemming:\\n{tokenizedNoStopLi[0]}\")\n",
    "print(f\"\\nSample sentence AFTER stemming:\\n{stemmedLi[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeList(stemmedList, ngramRangeStart, ngramRangeEnd):\n",
    "    '''\n",
    "    Creates a matrix of word vectors.\n",
    "    :param stemmedList: stemmed sentence list\n",
    "    :return: matrix of word vectors and vocabulary dictionary\n",
    "    '''\n",
    "    cv = CountVectorizer(binary=True, ngram_range=(ngramRangeStart, ngramRangeEnd))\n",
    "    cv.fit(stemmedList)\n",
    "    X = cv.transform(stemmedList)\n",
    "\n",
    "    return X, cv.vocabulary_\n",
    "    vectorizedTweets, vectorDictionary = vectorizeList(stemmedLi, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence #1:\n",
      "tiniebeani climat chang interest hustl global warm planet stop warm ye suv boom\n",
      "Sample sentence #2:\n",
      "rt natgeochannel watch beforetheflood right leodicaprio travel world tackl climat chang http co lkdehj3tnn httã\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample sentence #1:\\n{stemmedLi[0]}\")\n",
    "print(f\"Sample sentence #2:\\n{stemmedLi[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the sentence after stemming has lost some suffixes. \"Change\" became \"chang\", \"interesting\" became \"interest\" and \"warming\" became \"warm\". The array of words has also been converted back to one full sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
