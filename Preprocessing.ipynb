{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kristi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports, installation of flair required\n",
    "import pandas as pd\n",
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings, FlairEmbeddings\n",
    "from flair.data import Sentence\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import warnings\n",
    "import emoji\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "import spacy \n",
    "eng_lemmatizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re # regex\n",
    "\n",
    "#To save data in pkl file\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re, string\n",
    "\n",
    "fasttext = WordEmbeddings('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"./data/twitter_sentiment_data.csv\")\n",
    "\n",
    "def importpreprocessing(data):\n",
    "    data.drop(columns=\"tweetid\")\n",
    "    data.rename(columns={'sentiment':'label','message':'text'},inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Clean-up Text</h4>\n",
    "\n",
    "- Delete \"RT\"\n",
    "- Delete HTTPS/HTTP Links\n",
    "- Delete Double Whitespaces\n",
    "- Delete @twitteruser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    RE_TWITTERUSERS = re.compile(r'@\\S+', re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_HTTPS = re.compile(r'https\\S+', re.IGNORECASE)\n",
    "    RE_HTTP = re.compile(r'http\\S+', re.IGNORECASE)\n",
    "    RE_RT = re.compile(r'RT', re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TWITTERUSERS, \" \", text)\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "    text = re.sub(RE_HTTPS, \" \", text)\n",
    "    text = re.sub(RE_HTTP, \" \", text)\n",
    "    text = re.sub(RE_RT,\" \",text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Replace emojis with sentiment words\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokinize and Lemmatizise Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(data):\n",
    "    # generate new column with stopword free and lemmatizised text\n",
    "    data[\"shortData\"] = data['cleanData'].apply(lambda x:tokenize(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    # get the stopwords for the english language\n",
    "    my_stopwords = set(stopwords.words('english'))\n",
    "    lemmas = []\n",
    "    tokens = token_pattern.findall(text)\n",
    "    for item in tokens:\n",
    "        if item not in my_stopwords:\n",
    "            # all stems\n",
    "            doc = eng_lemmatizer(item)\n",
    "            for word in doc:\n",
    "                lemmas.append(word.lemma_)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raname columns and convert label -1 to 3 for conversion into labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minor_changes(data,name):\n",
    "    data.rename(columns={'shortData':'tokens'},inplace=True)\n",
    "\n",
    "    #replace -1 with 3 \n",
    "    data['label'] = data['label'].replace([-1],[3])\n",
    "\n",
    "    data.to_csv(f'preprocessed_{name}.csv')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for creating fasttext pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(doc):\n",
    "    if(isinstance(doc, str)):\n",
    "        help = doc[1:len(doc)-1]\n",
    "        help = help.replace('\\'', '')\n",
    "        as_list = help.split(',')\n",
    "        return as_list\n",
    "    else:\n",
    "        return doc\n",
    "\n",
    "def makefasttextpkl(data,name):\n",
    "    #drop all rows that we dont need\n",
    "    data.drop(columns=['Unnamed: 0','text','cleanData'])\n",
    "    vector_data = []\n",
    "\n",
    "    for t in data['tokens']:\n",
    "        word_list = get_list(t)\n",
    "        to_embed = Sentence(\" \".join(word_list))\n",
    "        fasttext.embed(to_embed)\n",
    "        vector = np.zeros((1, 300))\n",
    "        emb_list = []\n",
    "        for token in to_embed:\n",
    "            #print(token, token.embedding)\n",
    "            emb_list.append(token.embedding)\n",
    "        for v in emb_list:\n",
    "            v = v.detach().cpu().numpy()\n",
    "            vector += v\n",
    "        vector = vector/len(emb_list)\n",
    "        vector_data.append(vector)\n",
    "\n",
    "    print(len(vector_data))\n",
    "\n",
    "    data['fasttext'] = vector_data\n",
    "    data.to_pickle(f'fasttext_preprocessed_{name}.pkl')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(train, test):\n",
    "    # Preprocess train and test data\n",
    "    train = importpreprocessing(train)\n",
    "    test = importpreprocessing(test)\n",
    "\n",
    "    # Clean text data\n",
    "    train[\"cleanData\"] = train[\"text\"].map(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "    test[\"cleanData\"] = test[\"text\"].map(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Apply lemmatization\n",
    "    train = lemma(train)\n",
    "    test = lemma(test)\n",
    "\n",
    "    # Apply minor changes\n",
    "    train = minor_changes(train, \"train\")\n",
    "    test = minor_changes(test, \"test\")\n",
    "\n",
    "    # Create FastText pickle files with the required 'name' argument\n",
    "    train = makefasttextpkl(train, \"train\")\n",
    "    test = makefasttextpkl(test, \"test\")\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/trainset.csv\")\n",
    "test = pd.read_csv(\"./data/testset.csv\")\n",
    "\n",
    "wrapper(train,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
